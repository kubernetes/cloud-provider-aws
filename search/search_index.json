{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The AWS Cloud Provider for Kubernetes cloud-provider-aws The AWS cloud provider provides the interface between a Kubernetes cluster and AWS service APIs. This project allows a Kubernetes cluster to provision, monitor and remove AWS resources necessary for operation of the cluster. See the online documentation here Compatibility with Kubernetes The AWS cloud provider is released with a specific semantic version that correlates with the Kubernetes upstream version. The major and minor versions are equivalent to the compatible upstream release, and the patch version is reserved to denote subsequent releases of the cloud provider code for that Kubernetes release. Currently, for a given cloud provider release version, compatibility is ONLY guaranteed between that release and the corresponding Kubernetes version, meaning you need to upgrade the cloud provider components every time you upgrade Kubernetes, just like you would do for the kube controller manager. See the external cloud provider versioning KEP for more details. Kubernetes Version Latest AWS Cloud Provider Release Version v1.20 v1.20.0-alpha.0 v1.19 v1.19.0-alpha.1 v1.18 v1.18.0-alpha.1 Migration from In-Tree The in-tree cloud provider code has mostly stopped accepting new features, so future development for the AWS cloud provider should continue here. The in-tree plugins will be removed in a future release of Kubernetes. Components AWS Cloud Controller Manager The AWS Cloud Controller Manager is the controller that is primarily responsible for creating and updating AWS loadbalancers (classic and NLB) and node lifecycle management. The controller loops that are migrating out of the kube controller manager include the route controller, the service controller, the node controller, and the node lifecycle controller. See the cloud controller manager KEP for more details. AWS Credential Provider The AWS credential provider is a binary that is executed by kubelet to provide credentials for images in ECR. Refer to the credential provider extraction KEP for more details. Volume Plugins All the EBS volume plugin related logic will be in maintenance mode. For new feature request or bug fixes, please create issue or pull request in EBS CSI Driver","title":"Home"},{"location":"#cloud-provider-aws","text":"The AWS cloud provider provides the interface between a Kubernetes cluster and AWS service APIs. This project allows a Kubernetes cluster to provision, monitor and remove AWS resources necessary for operation of the cluster. See the online documentation here","title":"cloud-provider-aws"},{"location":"#compatibility-with-kubernetes","text":"The AWS cloud provider is released with a specific semantic version that correlates with the Kubernetes upstream version. The major and minor versions are equivalent to the compatible upstream release, and the patch version is reserved to denote subsequent releases of the cloud provider code for that Kubernetes release. Currently, for a given cloud provider release version, compatibility is ONLY guaranteed between that release and the corresponding Kubernetes version, meaning you need to upgrade the cloud provider components every time you upgrade Kubernetes, just like you would do for the kube controller manager. See the external cloud provider versioning KEP for more details. Kubernetes Version Latest AWS Cloud Provider Release Version v1.20 v1.20.0-alpha.0 v1.19 v1.19.0-alpha.1 v1.18 v1.18.0-alpha.1","title":"Compatibility with Kubernetes"},{"location":"#migration-from-in-tree","text":"The in-tree cloud provider code has mostly stopped accepting new features, so future development for the AWS cloud provider should continue here. The in-tree plugins will be removed in a future release of Kubernetes.","title":"Migration from In-Tree"},{"location":"#components","text":"","title":"Components"},{"location":"#aws-cloud-controller-manager","text":"The AWS Cloud Controller Manager is the controller that is primarily responsible for creating and updating AWS loadbalancers (classic and NLB) and node lifecycle management. The controller loops that are migrating out of the kube controller manager include the route controller, the service controller, the node controller, and the node lifecycle controller. See the cloud controller manager KEP for more details.","title":"AWS Cloud Controller Manager"},{"location":"#aws-credential-provider","text":"The AWS credential provider is a binary that is executed by kubelet to provide credentials for images in ECR. Refer to the credential provider extraction KEP for more details.","title":"AWS Credential Provider"},{"location":"#volume-plugins","text":"All the EBS volume plugin related logic will be in maintenance mode. For new feature request or bug fixes, please create issue or pull request in EBS CSI Driver","title":"Volume Plugins"},{"location":"CHANGELOG/","text":"v1.20.0-alpha.0 Add release github workflow ( #178 , @ayberk) Add script to generate changelog ( #179 , @ayberk) Replace book homepage with README ( #176 , @ayberk) update klog library from 2.4.0 to 2.5.0 ( #170 , @dineshkumar181094) Add documentation for cred provider ( #174 , @ayberk) Add released versions to README ( #175 , @nckturner) feat: Helm chart for aws cloud controller manager ( #173 , @JESWINKNINAN) Merge legacy provider ( #160 , @nckturner) Add wongma7 to owners ( #172 , @nckturner) tags: initial implementation of tags ( #149 , @nicolehanjing) Migrate to mkdocs ( #167 , @ayberk) Update Documentation ( #165 , @ayberk) Add ECR creds provider ( #157 , @ayberk) Pass in cloud config file to initialize cloud provider ( #164 , @nicolehanjing) Bump k8s.io/kubernetes@v1.20.0 ( #151 , @nicolehanjing) Fix cloudbuild image name ( #163 , @nckturner) Fix cloudbuild and simplify Makefile & cloudbuild ( #162 , @nckturner) Add docs publishing script and improve documentation ( #161 , @nckturner) Fix build command ( #159 , @ayberk) Makefile target to build and push image for release ( #138 , @nckturner) add verify-codegen in CI check ( #153 , @nicolehanjing) Add cloud config for tags ( #152 , @nicolehanjing) Bump the go version to v1.15 latest ( #150 , @nicolehanjing) Update go modules to include latest k8s.io/kubernetes module on v-1.19 ( #146 , @nicolehanjing) Update Flags section of README ( #147 , @ayberk) instances: initial implementation of instancesV2 interface ( #131 , @nicolehanjing) latest manifest should point to v1.19.0-alpha.1, not v1.19.1-alpha.1 ( #140 , @andrewsykim)","title":"Changelog"},{"location":"CHANGELOG/#v1200-alpha0","text":"Add release github workflow ( #178 , @ayberk) Add script to generate changelog ( #179 , @ayberk) Replace book homepage with README ( #176 , @ayberk) update klog library from 2.4.0 to 2.5.0 ( #170 , @dineshkumar181094) Add documentation for cred provider ( #174 , @ayberk) Add released versions to README ( #175 , @nckturner) feat: Helm chart for aws cloud controller manager ( #173 , @JESWINKNINAN) Merge legacy provider ( #160 , @nckturner) Add wongma7 to owners ( #172 , @nckturner) tags: initial implementation of tags ( #149 , @nicolehanjing) Migrate to mkdocs ( #167 , @ayberk) Update Documentation ( #165 , @ayberk) Add ECR creds provider ( #157 , @ayberk) Pass in cloud config file to initialize cloud provider ( #164 , @nicolehanjing) Bump k8s.io/kubernetes@v1.20.0 ( #151 , @nicolehanjing) Fix cloudbuild image name ( #163 , @nckturner) Fix cloudbuild and simplify Makefile & cloudbuild ( #162 , @nckturner) Add docs publishing script and improve documentation ( #161 , @nckturner) Fix build command ( #159 , @ayberk) Makefile target to build and push image for release ( #138 , @nckturner) add verify-codegen in CI check ( #153 , @nicolehanjing) Add cloud config for tags ( #152 , @nicolehanjing) Bump the go version to v1.15 latest ( #150 , @nicolehanjing) Update go modules to include latest k8s.io/kubernetes module on v-1.19 ( #146 , @nicolehanjing) Update Flags section of README ( #147 , @ayberk) instances: initial implementation of instancesV2 interface ( #131 , @nicolehanjing) latest manifest should point to v1.19.0-alpha.1, not v1.19.1-alpha.1 ( #140 , @andrewsykim)","title":"v1.20.0-alpha.0"},{"location":"RELEASE/","text":"AWS Cloud Provider Release Process NOTE: Your GitHub account must have the required permissions and you must have generated a GitHub token. Choosing the release version Using semantic versioning, pick a release number that makes sense by bumping the major, minor or patch release version. If its a major or minor release (backwards incompatible changes, and new features, respectively) then you will want to start this process with an alpha release first. Here are some examples: Bumping a minor version after releasing a new feature: v1.4.5 -> v1.5.0-alpha.0 After testing and allowing some time for feedback on the alpha, releasing v1.5.0: v1.4.5 -> v1.5.0 New patch release: v1.5.3 -> v1.5.4 New major version release with two alpha releases: v1.6.2 -> v2.0.0-alpha.0 -> v2.0.0-alpha.1 -> v2.0.0 Choosing the release branch You also might need to create a release branch, if it doesn't already exist, if this release requires backporting changes to an older major or minor version. For example, in the case that we are backporting a fix to the v0.5 release branch, and we have a v0.6 release branch (which we don't at the time of writing), then we would do the following: Create the release branch (named release-0.5) if it doesn't exist from the last v0.5.x tagged release (or check it out if it already exists). Cherry-pick the necessary commits onto the release branch. Follow the instructions below to create the release commit. Create a pull request to merge your fork of the release branch into the upstream release branch (i.e. /cloud-provider-aws/release-0.5 -> kubernetes/cloud-provider-aws/release-0.5). Create the release commit Generate the CHANGELOG We need to generate the CHANGELOG for the new release by running ./hack/changelog.py . You need to pass previous release tag to generate the changelog. python3 hack/changelog.py --token $GITHUB_TOKEN --changelog-file docs/CHANGELOG.md --section-title v1.20.0-alpha.1 --range v1.19.0-alpha.1.. This will prepend the changes to the CHANGELOG.md file. Update the README Search for any references to the previous version on the README, and update them if necessary. You'll need to update the compatibility table the least. Update the deployment files Update Helm values charts/aws-cloud-controller-manager/Chart.yaml charts/aws-cloud-controller-manager/values.yaml Update manifests/aws-cloud-controller-manager-daemonset.yaml Send a release PR At this point you should have all changes required for the release commit. Verify the changes via git diff and send a new PR with the release commit against the release branch. Note that if it doesn't exist, you'll need someone with write privileges to create it for you. Tag the release Once the PR is merged, pull the release branch locally and tag the release commit with the release tag. You'll need push privileges for this step. git checkout release-0.7 git pull upstream release-0.7 git tag v0.7.0 git push upstream v0.7.0 Verify the release on GitHub The new tag should trigger a new Github release. Verify that it has run by going to Releases . Then, click on the new version and verify all assets have been created: Source code (zip) Source code (tar.gz) Merge the release commit to the main branch Once the images are promoted, send a PR to merge the release commit to the main branch.","title":"Release Process"},{"location":"RELEASE/#aws-cloud-provider-release-process","text":"NOTE: Your GitHub account must have the required permissions and you must have generated a GitHub token.","title":"AWS Cloud Provider Release Process"},{"location":"RELEASE/#choosing-the-release-version","text":"Using semantic versioning, pick a release number that makes sense by bumping the major, minor or patch release version. If its a major or minor release (backwards incompatible changes, and new features, respectively) then you will want to start this process with an alpha release first. Here are some examples: Bumping a minor version after releasing a new feature: v1.4.5 -> v1.5.0-alpha.0 After testing and allowing some time for feedback on the alpha, releasing v1.5.0: v1.4.5 -> v1.5.0 New patch release: v1.5.3 -> v1.5.4 New major version release with two alpha releases: v1.6.2 -> v2.0.0-alpha.0 -> v2.0.0-alpha.1 -> v2.0.0","title":"Choosing the release version"},{"location":"RELEASE/#choosing-the-release-branch","text":"You also might need to create a release branch, if it doesn't already exist, if this release requires backporting changes to an older major or minor version. For example, in the case that we are backporting a fix to the v0.5 release branch, and we have a v0.6 release branch (which we don't at the time of writing), then we would do the following: Create the release branch (named release-0.5) if it doesn't exist from the last v0.5.x tagged release (or check it out if it already exists). Cherry-pick the necessary commits onto the release branch. Follow the instructions below to create the release commit. Create a pull request to merge your fork of the release branch into the upstream release branch (i.e. /cloud-provider-aws/release-0.5 -> kubernetes/cloud-provider-aws/release-0.5).","title":"Choosing the release branch"},{"location":"RELEASE/#create-the-release-commit","text":"","title":"Create the release commit"},{"location":"RELEASE/#generate-the-changelog","text":"We need to generate the CHANGELOG for the new release by running ./hack/changelog.py . You need to pass previous release tag to generate the changelog. python3 hack/changelog.py --token $GITHUB_TOKEN --changelog-file docs/CHANGELOG.md --section-title v1.20.0-alpha.1 --range v1.19.0-alpha.1.. This will prepend the changes to the CHANGELOG.md file.","title":"Generate the CHANGELOG"},{"location":"RELEASE/#update-the-readme","text":"Search for any references to the previous version on the README, and update them if necessary. You'll need to update the compatibility table the least.","title":"Update the README"},{"location":"RELEASE/#update-the-deployment-files","text":"Update Helm values charts/aws-cloud-controller-manager/Chart.yaml charts/aws-cloud-controller-manager/values.yaml Update manifests/aws-cloud-controller-manager-daemonset.yaml","title":"Update the deployment files"},{"location":"RELEASE/#send-a-release-pr","text":"At this point you should have all changes required for the release commit. Verify the changes via git diff and send a new PR with the release commit against the release branch. Note that if it doesn't exist, you'll need someone with write privileges to create it for you.","title":"Send a release PR"},{"location":"RELEASE/#tag-the-release","text":"Once the PR is merged, pull the release branch locally and tag the release commit with the release tag. You'll need push privileges for this step. git checkout release-0.7 git pull upstream release-0.7 git tag v0.7.0 git push upstream v0.7.0","title":"Tag the release"},{"location":"RELEASE/#verify-the-release-on-github","text":"The new tag should trigger a new Github release. Verify that it has run by going to Releases . Then, click on the new version and verify all assets have been created: Source code (zip) Source code (tar.gz)","title":"Verify the release on GitHub"},{"location":"RELEASE/#merge-the-release-commit-to-the-main-branch","text":"Once the images are promoted, send a PR to merge the release commit to the main branch.","title":"Merge the release commit to the main branch"},{"location":"TODO/","text":"TODO Prereqs Document required instance tags (i.e. KubernetesCluster: ) Load Balancers document all available label/annotations to configure ELBs/NLBs for Service Type=LoadBalancer Known Limitations Document limitation with hostname / private DNS? Kops Add a full example (ideally with IAM roles)","title":"TODO"},{"location":"TODO/#todo","text":"","title":"TODO"},{"location":"TODO/#prereqs","text":"Document required instance tags (i.e. KubernetesCluster: )","title":"Prereqs"},{"location":"TODO/#load-balancers","text":"document all available label/annotations to configure ELBs/NLBs for Service Type=LoadBalancer","title":"Load Balancers"},{"location":"TODO/#known-limitations","text":"Document limitation with hostname / private DNS?","title":"Known Limitations"},{"location":"TODO/#kops","text":"Add a full example (ideally with IAM roles)","title":"Kops"},{"location":"credential_provider/","text":"Credential Provider This feature is still in alpha and shouldn't be used in production environments. As part of the cloud provider extraction, KEP-2133 , proposed an extensible way to fetch credentials for pulling images. When kubelet needs credentials to fetch an image, it will now invoke a plugin based on the configuration provided by the cluster operator. Please see the original KEP for details. We currently have the implementation for fetching ECR credentials. In order to use this new plugin, you'll have to Pass the folder where the binary is located as --image-credential-provider-bin-dir to the kubelet. Create a new CredentialProviderConfig and pass its location to the kubelet via --image-credential-provider-config . Example config: { \"providers\": [ { \"name\": \"ecr-credential-provider\", \"matchImages\" : [ \"*.dkr.ecr.*.amazonaws.com, \"*.dkr.ecr.*.amazonaws.com.cn, ], \"apiVersion\": \"credentialprovider.kubelet.k8s.io/v1alpha1\", \"defaultCacheDuration\": \"0\" } ] } Once you pass this config to the kubelet, everytime it needs to fetch an image that matches one of the \"matchImages\" patterns, it will invoke the \"ecr-credential-provider\" binary in the --image-credential-provider-bin-dir folder. In turn, the plugin will fetch the credentials for kubelet and send it back via stdio. Note that the name of the \"provider\" in your config has to match the name of the binary.","title":"Credential Provider"},{"location":"credential_provider/#credential-provider","text":"This feature is still in alpha and shouldn't be used in production environments. As part of the cloud provider extraction, KEP-2133 , proposed an extensible way to fetch credentials for pulling images. When kubelet needs credentials to fetch an image, it will now invoke a plugin based on the configuration provided by the cluster operator. Please see the original KEP for details. We currently have the implementation for fetching ECR credentials. In order to use this new plugin, you'll have to Pass the folder where the binary is located as --image-credential-provider-bin-dir to the kubelet. Create a new CredentialProviderConfig and pass its location to the kubelet via --image-credential-provider-config . Example config: { \"providers\": [ { \"name\": \"ecr-credential-provider\", \"matchImages\" : [ \"*.dkr.ecr.*.amazonaws.com, \"*.dkr.ecr.*.amazonaws.com.cn, ], \"apiVersion\": \"credentialprovider.kubelet.k8s.io/v1alpha1\", \"defaultCacheDuration\": \"0\" } ] } Once you pass this config to the kubelet, everytime it needs to fetch an image that matches one of the \"matchImages\" patterns, it will invoke the \"ecr-credential-provider\" binary in the --image-credential-provider-bin-dir folder. In turn, the plugin will fetch the credentials for kubelet and send it back via stdio. Note that the name of the \"provider\" in your config has to match the name of the binary.","title":"Credential Provider"},{"location":"development/","text":"Development A local single node cluster can be brought up on AWS by running the local up script while on an AWS EC2 instance. Before running this, ensure that the instance you are running on has the KubernetesCluster tag. The tag can be any value. ./hack/local-up-cluster.sh By default this script will use the cloud provider binary from this repository. You will need to have the k8s main repo cloned before running this script.","title":"Running the Cloud Provider"},{"location":"development/#development","text":"A local single node cluster can be brought up on AWS by running the local up script while on an AWS EC2 instance. Before running this, ensure that the instance you are running on has the KubernetesCluster tag. The tag can be any value. ./hack/local-up-cluster.sh By default this script will use the cloud provider binary from this repository. You will need to have the k8s main repo cloned before running this script.","title":"Development"},{"location":"getting_started/","text":"Getting Started Before you start, make sure you go through the prerequisites . In order to launch a cluster running the aws-cloud-controller-manager, you can run the appropriate container image release from this repository on an existing cluster, or you can use a deployment tool that has support for deploying it, like kops. Running on an Existing Cluster Follow these steps when upgrading an existing cluster by launching the aws-cloud-controller-manager as a pod: Temporarily stop the kube-controller-managers from running. Add the --cloud-provider=external to the kube-controller-manager config. Add the --cloud-provider=external to the kube-apiserver config. Add the --cloud-provider=external to each the kubelet's config. Deploy the required RBAC to your cluster: kubectl apply -f https://raw.githubusercontent.com/kubernetes/cloud-provider-aws/master/manifests/rbac.yaml Deploy the cloud-controller-manager to your cluster: kubectl apply -f https://raw.githubusercontent.com/kubernetes/cloud-provider-aws/master/manifests/aws-cloud-controller-manager-daemonset.yaml Flags flag component description --cloud-provider=external kube-apiserver Disables the cloud provider in the API Server. --cloud-provider=external kube-controller-manager Disables the cloud provider in the Kube Controller Manager. --cloud-provider=external kubelet Disables the cloud provider in the Kubelet. --cloud-provider=[aws|aws/v2] aws-cloud-controller-manager Optional. Selects the legacy cloud-provider or the v2 cloud-provider in the aws-cloud-controller-manager. WARNING: the v2 cloud-provider is in a pre-alpha state. --external-cloud-volume-plugin=aws kube-controller-manager Tells the Kube Controller Manager to run the volume loops that have cloud provider code in them. This is required for volumes to work if you are not using CSI with migration enabled. Using Kops In order to create a cluster using kops, the following flags should be set in your cluster.yaml in order to pass the correct flags to the control plane components. apiVersion: kops.k8s.io/v1alpha2 kind: Cluster metadata: name: cloud-controller-example spec: cloudControllerManager: cloudProvider: aws kubeControllerManager: externalCloudVolumePlugin: aws cloudProvider: aws kubeAPIServer: cloudProvider: external kubelet: cloudProvider: aws Note: the above config omits all config not related to the aws-cloud-controller-manager Check examples/kops for a full kops configuration. (TODO)","title":"Getting Started"},{"location":"getting_started/#getting-started","text":"Before you start, make sure you go through the prerequisites . In order to launch a cluster running the aws-cloud-controller-manager, you can run the appropriate container image release from this repository on an existing cluster, or you can use a deployment tool that has support for deploying it, like kops.","title":"Getting Started"},{"location":"getting_started/#running-on-an-existing-cluster","text":"Follow these steps when upgrading an existing cluster by launching the aws-cloud-controller-manager as a pod: Temporarily stop the kube-controller-managers from running. Add the --cloud-provider=external to the kube-controller-manager config. Add the --cloud-provider=external to the kube-apiserver config. Add the --cloud-provider=external to each the kubelet's config. Deploy the required RBAC to your cluster: kubectl apply -f https://raw.githubusercontent.com/kubernetes/cloud-provider-aws/master/manifests/rbac.yaml Deploy the cloud-controller-manager to your cluster: kubectl apply -f https://raw.githubusercontent.com/kubernetes/cloud-provider-aws/master/manifests/aws-cloud-controller-manager-daemonset.yaml","title":"Running on an Existing Cluster"},{"location":"getting_started/#flags","text":"flag component description --cloud-provider=external kube-apiserver Disables the cloud provider in the API Server. --cloud-provider=external kube-controller-manager Disables the cloud provider in the Kube Controller Manager. --cloud-provider=external kubelet Disables the cloud provider in the Kubelet. --cloud-provider=[aws|aws/v2] aws-cloud-controller-manager Optional. Selects the legacy cloud-provider or the v2 cloud-provider in the aws-cloud-controller-manager. WARNING: the v2 cloud-provider is in a pre-alpha state. --external-cloud-volume-plugin=aws kube-controller-manager Tells the Kube Controller Manager to run the volume loops that have cloud provider code in them. This is required for volumes to work if you are not using CSI with migration enabled.","title":"Flags"},{"location":"getting_started/#using-kops","text":"In order to create a cluster using kops, the following flags should be set in your cluster.yaml in order to pass the correct flags to the control plane components. apiVersion: kops.k8s.io/v1alpha2 kind: Cluster metadata: name: cloud-controller-example spec: cloudControllerManager: cloudProvider: aws kubeControllerManager: externalCloudVolumePlugin: aws cloudProvider: aws kubeAPIServer: cloudProvider: external kubelet: cloudProvider: aws Note: the above config omits all config not related to the aws-cloud-controller-manager Check examples/kops for a full kops configuration. (TODO)","title":"Using Kops"},{"location":"prerequisites/","text":"Prerequisites IAM Policies For the aws-cloud-controller-manager to be able to communicate to AWS APIs, you will need to create a few IAM policies for your EC2 instances. The control plane (formerly master) policy is a bit open and can be scaled back depending on the use case. Adjust these based on your needs. Control Plane Policy { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"autoscaling:DescribeAutoScalingGroups\", \"autoscaling:DescribeLaunchConfigurations\", \"autoscaling:DescribeTags\", \"ec2:DescribeInstances\", \"ec2:DescribeRegions\", \"ec2:DescribeRouteTables\", \"ec2:DescribeSecurityGroups\", \"ec2:DescribeSubnets\", \"ec2:DescribeVolumes\", \"ec2:CreateSecurityGroup\", \"ec2:CreateTags\", \"ec2:CreateVolume\", \"ec2:ModifyInstanceAttribute\", \"ec2:ModifyVolume\", \"ec2:AttachVolume\", \"ec2:AuthorizeSecurityGroupIngress\", \"ec2:CreateRoute\", \"ec2:DeleteRoute\", \"ec2:DeleteSecurityGroup\", \"ec2:DeleteVolume\", \"ec2:DetachVolume\", \"ec2:RevokeSecurityGroupIngress\", \"ec2:DescribeVpcs\", \"elasticloadbalancing:AddTags\", \"elasticloadbalancing:AttachLoadBalancerToSubnets\", \"elasticloadbalancing:ApplySecurityGroupsToLoadBalancer\", \"elasticloadbalancing:CreateLoadBalancer\", \"elasticloadbalancing:CreateLoadBalancerPolicy\", \"elasticloadbalancing:CreateLoadBalancerListeners\", \"elasticloadbalancing:ConfigureHealthCheck\", \"elasticloadbalancing:DeleteLoadBalancer\", \"elasticloadbalancing:DeleteLoadBalancerListeners\", \"elasticloadbalancing:DescribeLoadBalancers\", \"elasticloadbalancing:DescribeLoadBalancerAttributes\", \"elasticloadbalancing:DetachLoadBalancerFromSubnets\", \"elasticloadbalancing:DeregisterInstancesFromLoadBalancer\", \"elasticloadbalancing:ModifyLoadBalancerAttributes\", \"elasticloadbalancing:RegisterInstancesWithLoadBalancer\", \"elasticloadbalancing:SetLoadBalancerPoliciesForBackendServer\", \"elasticloadbalancing:AddTags\", \"elasticloadbalancing:CreateListener\", \"elasticloadbalancing:CreateTargetGroup\", \"elasticloadbalancing:DeleteListener\", \"elasticloadbalancing:DeleteTargetGroup\", \"elasticloadbalancing:DescribeListeners\", \"elasticloadbalancing:DescribeLoadBalancerPolicies\", \"elasticloadbalancing:DescribeTargetGroups\", \"elasticloadbalancing:DescribeTargetHealth\", \"elasticloadbalancing:ModifyListener\", \"elasticloadbalancing:ModifyTargetGroup\", \"elasticloadbalancing:RegisterTargets\", \"elasticloadbalancing:DeregisterTargets\", \"elasticloadbalancing:SetLoadBalancerPoliciesOfListener\", \"iam:CreateServiceLinkedRole\", \"kms:DescribeKey\" ], \"Resource\": [ \"*\" ] } ] } Node Policy { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"ec2:DescribeInstances\", \"ec2:DescribeRegions\", \"ecr:GetAuthorizationToken\", \"ecr:BatchCheckLayerAvailability\", \"ecr:GetDownloadUrlForLayer\", \"ecr:GetRepositoryPolicy\", \"ecr:DescribeRepositories\", \"ecr:ListImages\", \"ecr:BatchGetImage\" ], \"Resource\": \"*\" } ] }","title":"Prerequisites"},{"location":"prerequisites/#prerequisites","text":"","title":"Prerequisites"},{"location":"prerequisites/#iam-policies","text":"For the aws-cloud-controller-manager to be able to communicate to AWS APIs, you will need to create a few IAM policies for your EC2 instances. The control plane (formerly master) policy is a bit open and can be scaled back depending on the use case. Adjust these based on your needs. Control Plane Policy { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"autoscaling:DescribeAutoScalingGroups\", \"autoscaling:DescribeLaunchConfigurations\", \"autoscaling:DescribeTags\", \"ec2:DescribeInstances\", \"ec2:DescribeRegions\", \"ec2:DescribeRouteTables\", \"ec2:DescribeSecurityGroups\", \"ec2:DescribeSubnets\", \"ec2:DescribeVolumes\", \"ec2:CreateSecurityGroup\", \"ec2:CreateTags\", \"ec2:CreateVolume\", \"ec2:ModifyInstanceAttribute\", \"ec2:ModifyVolume\", \"ec2:AttachVolume\", \"ec2:AuthorizeSecurityGroupIngress\", \"ec2:CreateRoute\", \"ec2:DeleteRoute\", \"ec2:DeleteSecurityGroup\", \"ec2:DeleteVolume\", \"ec2:DetachVolume\", \"ec2:RevokeSecurityGroupIngress\", \"ec2:DescribeVpcs\", \"elasticloadbalancing:AddTags\", \"elasticloadbalancing:AttachLoadBalancerToSubnets\", \"elasticloadbalancing:ApplySecurityGroupsToLoadBalancer\", \"elasticloadbalancing:CreateLoadBalancer\", \"elasticloadbalancing:CreateLoadBalancerPolicy\", \"elasticloadbalancing:CreateLoadBalancerListeners\", \"elasticloadbalancing:ConfigureHealthCheck\", \"elasticloadbalancing:DeleteLoadBalancer\", \"elasticloadbalancing:DeleteLoadBalancerListeners\", \"elasticloadbalancing:DescribeLoadBalancers\", \"elasticloadbalancing:DescribeLoadBalancerAttributes\", \"elasticloadbalancing:DetachLoadBalancerFromSubnets\", \"elasticloadbalancing:DeregisterInstancesFromLoadBalancer\", \"elasticloadbalancing:ModifyLoadBalancerAttributes\", \"elasticloadbalancing:RegisterInstancesWithLoadBalancer\", \"elasticloadbalancing:SetLoadBalancerPoliciesForBackendServer\", \"elasticloadbalancing:AddTags\", \"elasticloadbalancing:CreateListener\", \"elasticloadbalancing:CreateTargetGroup\", \"elasticloadbalancing:DeleteListener\", \"elasticloadbalancing:DeleteTargetGroup\", \"elasticloadbalancing:DescribeListeners\", \"elasticloadbalancing:DescribeLoadBalancerPolicies\", \"elasticloadbalancing:DescribeTargetGroups\", \"elasticloadbalancing:DescribeTargetHealth\", \"elasticloadbalancing:ModifyListener\", \"elasticloadbalancing:ModifyTargetGroup\", \"elasticloadbalancing:RegisterTargets\", \"elasticloadbalancing:DeregisterTargets\", \"elasticloadbalancing:SetLoadBalancerPoliciesOfListener\", \"iam:CreateServiceLinkedRole\", \"kms:DescribeKey\" ], \"Resource\": [ \"*\" ] } ] } Node Policy { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"ec2:DescribeInstances\", \"ec2:DescribeRegions\", \"ecr:GetAuthorizationToken\", \"ecr:BatchCheckLayerAvailability\", \"ecr:GetDownloadUrlForLayer\", \"ecr:GetRepositoryPolicy\", \"ecr:DescribeRepositories\", \"ecr:ListImages\", \"ecr:BatchGetImage\" ], \"Resource\": \"*\" } ] }","title":"IAM Policies"}]}